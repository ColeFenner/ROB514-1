{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "requirements: ../requirements.txt\n",
    "solutions_pdf: true\n",
    "export_cell:\n",
    "    instructions: \"Submit through gradescope, lecture activity 1 Arrays. Be sure to read the info on the autograder before submitting.\"\n",
    "    pdf: true\n",
    "    run_tests: true\n",
    "    show_question_points: true\n",
    "generate: \n",
    "    pdf: true\n",
    "    filtering: true\n",
    "    pagebreaks: true\n",
    "    zips: false\n",
    "    points_possible: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture goals\n",
    "\n",
    "1. Understand the benefit of numpy (over lists) for operating over lists of numbers\n",
    "2. Introduction to numpy-style array operations\n",
    "3. Dictionaries for data encapsulation\n",
    "4. Functions for functionality encapsulation\n",
    "\n",
    "## Functions: \n",
    "Functions enable encapsulation of, well, functionality.\n",
    "\n",
    "They're also a useful mental tool for organizing and structuring your thoughts on how to solve a given problem\n",
    "1. Clearly define a bit of code that takes in some inputs, does some computation, then outputs some data\n",
    "2. Makes it easier to test that code with different inputs\n",
    "3. Practicalities: Prevents one of the most common sources of errors - re-using variable names\n",
    "\n",
    "It's almost never wrong to encapsulate a bit of code in a function. It can slow down (a tiny bit) computation time, but can greatly reduce debugging time, so it's usually worth it.\n",
    "\n",
    "Python's function syntax is beautifully designed to make it easy to set default values for parameters and pass back as much data as you want. We'll see more of that later; for this assignment we'll use the power of dictionaries to pass back \"labeled\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access all numpy functions as np.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: list\n",
    "points: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Stats on a list\n",
    "\n",
    "## calculate stats on a list\n",
    "\n",
    "TODO: in calc_stats_from_list function \n",
    "- Calculate the mean of the negative and positive values\n",
    "- Count the total number of negative/positive values\n",
    "- Store the values in a dictionary\n",
    "\n",
    "This function calculates the given stats from the list that is passed in. There is test code below this function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION NO PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats_from_list(in_list):\n",
    "    \"\"\" Calculate mean of positive numbers, mean of negatives numbers\n",
    "    Separate the list into positive and negative numbers. Calculate the mean of each. Return those means, along with\n",
    "     how many positive/negative numbers there were\n",
    "    @param in_list : any list type\n",
    "    @return - A dictionary with the desired stats\"\"\"\n",
    "\n",
    "    # These are the stats we're calculating. This is more elegant/useful than creating four variables - it keeps all\n",
    "    #  of the values in the same place and assigns a meaningful label (key) to them\n",
    "    dict_ret_stats = {\"Mean positive\": 0.0, \"Mean negative\": -0.0, \"Count positive\": 0, \"Count negative\": 0}\n",
    "\n",
    "    # TODO: \n",
    "    #   Calculate the means and the counts and store them in the dictionary with the keys given above in dict_ret_stats\n",
    "    #   You'll need a for loop to go over the list and an if statement to separate into positive and negative\n",
    "    # BEGIN SOLUTION\n",
    "    # Note that I would normally do this with 4 variables, and then create the dictionary at the end and return it,\n",
    "    #   (which would save a lot of dictionary accesses) but doing this way because it makes setting up the automatic\n",
    "    #   grading software easier\n",
    "    dict_ret_stats[\"Mean positive\"] = 0.0\n",
    "    dict_ret_stats[\"Mean negative\"] = 0.0\n",
    "    for n in in_list:\n",
    "        if n < 0:\n",
    "            dict_ret_stats[\"Mean negative\"] += n\n",
    "            dict_ret_stats[\"Count negative\"] += 1\n",
    "        else:\n",
    "            dict_ret_stats[\"Mean positive\"] += n\n",
    "            dict_ret_stats[\"Count positive\"] += 1\n",
    "\n",
    "    # Cool tricks you can do when variables are dictionary names that are strings\n",
    "    for s in [\"positive\", \"negative\"]:\n",
    "        if dict_ret_stats[\"Count \" + s] > 0:\n",
    "            dict_ret_stats[\"Mean \" + s] /= dict_ret_stats[\"Count \" + s]\n",
    "    # END SOLUTION\n",
    "    return dict_ret_stats\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for list\n",
    "\n",
    "Create the arrays and test them. Here's another advantage of functions - you can create test data for yourself to make sure the code is working right. Encapsulating the code in a function means you don't \n",
    "1. Accidentally change the code when switching from the test data to the real data\n",
    "2. You can make more than one test \n",
    "3. You can run the tests more than once/all the time to double check that you didn't \"break\" the code\n",
    "\n",
    "TODO: \n",
    "- Fill in the calc_stats_from_list function above\n",
    "- Run the cell below - it will print out if your values are incorrect\n",
    "\n",
    "Note that, below, we'll test this code one last time with randomly generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n",
    "test_list_one = [-0.75, -0.25, 1.0 / 3.0, 2.0 / 3.0, 3.0 / 3.0]\n",
    "test_list_res = calc_stats_from_list(test_list_one)\n",
    "\n",
    "b_tests_passed = True\n",
    "if not np.isclose(test_list_res[\"Mean positive\"], 2.0 / 3.0):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean positive is not correct, should be {2.0/3.0}, got {test_list_res['Mean positive']}\")\n",
    "\n",
    "if not np.isclose(test_list_res[\"Mean negative\"], -0.5):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean negative is not correct, should be -0.5, got {test_list_res['Mean negative']}\")\n",
    "\n",
    "if test_list_res[\"Count positive\"] is not 3:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 3, got {test_list_res['Count positive']}\")\n",
    "\n",
    "if test_list_res[\"Count negative\"] is not 2:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 2, got {test_list_res['Count negative']}\")\n",
    "\n",
    "if b_tests_passed:\n",
    "    print(\"All array tests passed!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_stats_from_list([-1, 2, -3, 4, -5])\n",
    "assert np.isclose(res[\"Mean positive\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_stats_from_list([-1, 2, -3, 4, -5])\n",
    "assert np.isclose(res[\"Mean negative\"], -3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_stats_from_list([-1, 2, -3, 4, -5])\n",
    "assert np.isclose(res[\"Count positive\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = calc_stats_from_list([-1, 2, -3, 4, -5])\n",
    "assert np.isclose(res[\"Count negative\"], 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: nparray\n",
    "points: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Doing it again with a numpy array\n",
    "\n",
    "## Fill in calc_stats_from_nparray\n",
    "\n",
    "For this function, assume the input is an numpy array.\n",
    "\n",
    "TODO: Same as the previous question, but this time do it for a numpy array in calc_stats_from_nparray\n",
    "- NO **if** statements or **for** loops - do this all with numpy operations\n",
    "\n",
    "You might find \"count_nonzero\" useful.\n",
    "\n",
    "As before, test code is below"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_stats_from_nparray(in_nparray):\n",
    "    \"\"\" Calculate mean of positive numbers, mean of negatives numbers\n",
    "    Separate the list into positive and negative numbers. Calculate the mean of each. Return those means, along with\n",
    "     how many positive/negative numbers there were\n",
    "    @param in_list : numpy array\n",
    "    @return - A dictionary with the desired stats\"\"\"\n",
    "\n",
    "    # These are the stats we're calculating. This is more elegant/useful than creating four variables - it keeps all\n",
    "    #  of the values in the same place and assigns a meaningful label (key) to them\n",
    "    dict_ret_stats = {\"Mean positive\": 0, \"Mean negative\": 0, \"Count positive\": 0, \"Count negative\": 0}\n",
    "\n",
    "    # TODO: Calculate the mean for the positive and the negative values\n",
    "    #.  Also count the number of each\n",
    "    #   Do NOT use a for loop - use boolean indexing\n",
    "    # BEGIN SOLUTION \n",
    "    dict_ret_stats = {\"Mean positive\": np.mean(in_nparray[in_nparray >= 0]),\n",
    "                      \"Mean negative\": np.mean(in_nparray[in_nparray < 0]),\n",
    "                      \"Count positive\": np.count_nonzero(in_nparray >= 0),\n",
    "                      \"Count negative\": np.count_nonzero(in_nparray < 0)}\n",
    "    # END SOLUTION\n",
    "    return dict_ret_stats"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test code for numpy array\n",
    "There is a \"fancy\" way to do this test with the second function without duplicating code, but it's confusing, so... we'll just duplicate it here. This will print out if your function above is returning incorrect values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nparray_one = np.array(test_list_one)  # Convert the previous test list to a numpy array\n",
    "test_list_res = calc_stats_from_nparray(test_nparray_one)\n",
    "\n",
    "b_tests_passed = True\n",
    "if not np.isclose(test_list_res[\"Mean positive\"], 2.0 / 3.0):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean positive is not correct, should be {2.0/3.0}, got {test_list_res['Mean positive']}\")\n",
    "\n",
    "if not np.isclose(test_list_res[\"Mean negative\"], -0.5):\n",
    "    b_tests_passed = False\n",
    "    print(f\"Mean negative is not correct, should be -0.5, got {test_list_res['Mean negative']}\")\n",
    "\n",
    "if test_list_res[\"Count positive\"] is not 3:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 3, got {test_list_res['Count positive']}\")\n",
    "\n",
    "if test_list_res[\"Count negative\"] is not 2:\n",
    "    b_tests_passed = False\n",
    "    print(f\"Count positive numbers, should be 2, got {test_list_res['Count negative']}\")\n",
    "\n",
    "if b_tests_passed:\n",
    "    print(\"All numpy array tests passed!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([-1, 2, -3, 4, -5])\n",
    "res = calc_stats_from_nparray(test_data)\n",
    "assert np.isclose(res[\"Mean positive\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([-1, 2, -3, 4, -5])\n",
    "res = calc_stats_from_nparray(test_data)\n",
    "assert np.isclose(res[\"Mean negative\"], -3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([-1, 2, -3, 4, -5])\n",
    "res = calc_stats_from_nparray(test_data)\n",
    "assert np.isclose(res[\"Count positive\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array([-1, 2, -3, 4, -5])\n",
    "res = calc_stats_from_nparray(test_data)\n",
    "assert np.isclose(res[\"Count negative\"], 3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: tests\n",
    "points: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that it all works\n",
    "\n",
    "TODO: If both your functions are correct then the tests below should only output \"Done test\". Otherwise, it will output which values were different\n",
    "\n",
    "This next cell is a function to generate random values to test with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit of code will generate a list or numpy array with random positive and negative values.\n",
    "def create_data(n_data=10, b_ret_numpy=True):\n",
    "    \"\"\" Create a random mix of positive and negative numbers\n",
    "    @param n_data - how big to make the list/array\n",
    "    @param b_ret_numpy - return a list or a numpy array\n",
    "    @return the list or numpy array\"\"\"\n",
    "    my_data = np.random.random_sample(n_data)\n",
    "\n",
    "    n_to_convert = np.random.randint(low=1, high=n_data-1)\n",
    "    n_convert = np.random.randint(low=0, high=n_data-1, size=n_to_convert)\n",
    "    my_data[n_convert] *= -1.0\n",
    "\n",
    "    if b_ret_numpy is False:\n",
    "        return list(my_data)\n",
    "    return my_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually do the tests\n",
    "\n",
    "Check by comparing the results of the list-based function against the numpy-based one (doesn't guarantee it's right, \n",
    "but...). Try 10 times. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't care what the iteration is over so use _ to say \"we don't need a variable\"\n",
    "b_tests_passed = True\n",
    "for _ in range(0, 10):\n",
    "    # Get some random data\n",
    "    test_data = create_data()\n",
    "    test_data_list = list(test_data)  # Notice the cast to a list type\n",
    "\n",
    "    # Call your two functions with the random data.\n",
    "    res_list = calc_stats_from_list(test_data_list)\n",
    "    res_np = calc_stats_from_nparray(test_data)\n",
    "\n",
    "    # For all four stored values...\n",
    "    for k, v in res_list.items():\n",
    "        # Use isclose instead of == because two of these are floating point values, and == never works with\n",
    "        #  floating point values\n",
    "        # Since we used the same keys (names) in the two different dictionaries, we can pass that key to the other\n",
    "        #   dictionary\n",
    "        if not np.isclose(res_np[k], v):\n",
    "            b_tests_passed = False\n",
    "            print(f\"Returned different values {k}, {v} and {res_np[k]}\")\n",
    "    if np.isclose(res_list[\"Mean positive\"], 0.0):\n",
    "        b_tests_passed = False\n",
    "        print(f\"List not implemented\\n\")\n",
    "    if np.isclose(res_np[\"Mean positive\"], 0.0):\n",
    "        b_tests_passed = False\n",
    "        print(f\"Numpy not implemented\\n\")\n",
    "        \n",
    "if b_tests_passed:\n",
    "    print(\"Comparison test passed\")\n",
    "# BEGIN SOLUTION NO PROMPT\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = create_data()\n",
    "test_data_list = list(test_data)\n",
    "res_list = calc_stats_from_list(test_data_list)\n",
    "res_np = calc_stats_from_nparray(test_data)\n",
    "for k, v in res_list.items():\n",
    "    if not np.isclose(res_np[k], v):\n",
    "        print(f\"Values different {res_np[k]} {v}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN QUESTION\n",
    "name: hours_collaborators\n",
    "points: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hours and collaborators\n",
    "Required for every assignment - fill out before you hand-in.\n",
    "\n",
    "Listing names and websites helps you to document who you worked with and what internet help you received in the case of any plagiarism issues. You should list names of anyone (in class or not) who has substantially helped you with an assignment - or anyone you have *helped*. You do not need to list TAs.\n",
    "\n",
    "Listing hours helps us track if the assignments are too long."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION NO PROMPT\n",
    "worked_with_names = {}\n",
    "websites = {}\n",
    "hours = 4\n",
    "\n",
    "# END SOLUTION\n",
    "\n",
    "\"\"\" # BEGIN PROMPT\n",
    "# List of names (creates a set)\n",
    "worked_with_names = {\"not filled out\"}\n",
    "# List of URLS (creates a set)\n",
    "websites = {\"not filled out\"}\n",
    "# Approximate number of hours, including lab/in-class time\n",
    "hours = -1.5\n",
    "\n",
    "# for all row, column in all_indices_from_where\n",
    "#.   if this is the column for wrist torque \n",
    "#.      print(f\"Row: {r}, Time step: {c // n_time_steps} Successful y/n: {pick_data[r, -1] == 1}, value: {pick_data[r, c]}\")\n",
    "\"\"\" # END PROMPT"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# BEGIN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not \"not filled out\" in worked_with_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(not \"not filled out\" in websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hours > 0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END TESTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# END QUESTION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
